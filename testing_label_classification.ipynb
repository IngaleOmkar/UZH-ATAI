{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/atai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from embeddings import EmbeddingsResponder\n",
    "from entity_extraction import Extractor\n",
    "from factual import FactualResponder\n",
    "from data_repository import DataRepository\n",
    "from intent_classifier import IntentClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Initializing Data Repository ==========\n",
      "========== Loading NER Embeddings ==========\n",
      "========== Loading graph ==========\n",
      "========== Loading data for factual QA ==========\n",
      "========== Loading data for embeddings ==========\n",
      "========== Data Repository initialized ==========\n"
     ]
    }
   ],
   "source": [
    "data_repository = DataRepository()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "intent_classifier = IntentClassifier(data_repository)\n",
    "extractor = Extractor(data_repository)\n",
    "embeddings = EmbeddingsResponder(data_repository, extractor, intent_classifier)\n",
    "factual = FactualResponder(data_repository, extractor, intent_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cast member', 'MPAA film rating', 'FSK film rating', 'BBFC rating', 'RARS rating', 'Kijkwijzer rating', 'ClassInd rating', 'INCAA film rating', 'IMDb ID', 'KMRB film rating', 'FPB rating', 'KAVI rating', 'MedierÃ¥det rating', 'RCQ classification', 'OFLC classification', 'IFCO rating', 'applies to jurisdiction', 'field of work', 'conferred by', 'crew member(s)', 'native language', 'director / manager', 'relative', 'film editor', 'medical condition', 'product or material produced', 'occupation', 'student of', 'employer', 'from narrative universe', 'founded by', 'political ideology', 'home world', 'place of burial', 'manner of death', 'publisher', 'facet of', 'owned by', 'depicted by', 'located in the administrative territorial entity', 'partner in business or sport', 'described by source', 'participant in', 'winner', 'movement', 'genre', 'replaces', 'replaced by', 'operator', 'capital of', 'named after', 'partially coincident with', 'convicted of', 'religion', 'nominated for', 'languages spoken, written or signed', 'affiliation', 'has pet', 'executive producer', 'published in', 'takes place in fictional universe', 'based on', 'present in work', 'fictional universe described in', 'list of works', 'contains administrative territorial entity', 'country for sport', 'has effect', 'follows', 'has quality', 'followed by', 'killed by', 'lifestyle', 'lowest point', 'headquarters location', 'award received', 'country', 'creator', 'ethnic group', 'performer', 'developer', 'part of the series', 'image', 'depicts', 'choreographer', 'owner of', 'represented by', 'after a work by', 'different from', 'place of birth', 'twinned administrative body', 'collection', 'health specialty', 'place of death', 'basin country', 'located in or next to body of water', 'aspect ratio', 'fabrication method', 'sex or gender', 'box office', 'father', 'uses', 'indigenous to', 'time period', 'intended public', 'NMHH film rating', 'set in period', 'military branch', 'sports discipline competed in', 'narrator', 'mother', 'costume designer', 'operating area', 'sidekick of', 'production designer', 'superhuman feature or ability', 'spouse', 'official residence', 'place of detention', 'interested in', 'country of citizenship', 'production company', 'Filmiroda rating', 'copyright license', 'EIRIN film rating', 'CNC film rating (France)', 'location', 'subclass of', 'cites work', 'designed by', 'place of publication', 'language used', 'continent', 'operating system', 'film crew member', 'practiced by', 'instance of', 'Australian Classification', 'art director', 'storyboard artist', 'musical conductor', 'broadcast by', 'ICAA rating', 'significant person', 'sibling', 'CNC film rating (Romania)', 'director of photography', 'stepparent', 'head of state', 'capital', 'part of', 'original language of film or TV show', 'JMK film rating', 'use', 'official language', 'presenter', 'original film format', 'RTC film rating', 'located in present-day administrative territorial entity', 'copyright holder', 'child', 'platform', 'language of work or name', 'field of this occupation', 'distribution format', 'original broadcaster', 'unmarried partner', 'industry', 'first appearance', 'said to be the same as', 'scenographer', 'opposite of', 'color', 'member of', 'occupant', 'shares border with', 'input method', 'make-up artist', 'archives at', 'season', 'country of origin', 'derivative work', 'author', 'assessment', 'sound designer', 'presented in', 'cause of death', 'member of the crew of', 'IGAC rating', 'IMDA rating', 'has part', 'diplomatic relation', 'permanent resident of', 'member of sports team', 'residence', 'director', 'publication date', 'screenwriter', 'plot expanded in', 'head of government', 'conflict', 'quotes work', 'copyright status', 'copyright representative', 'edition or translation of', 'has works in the collection', 'sport', 'ancestral home', 'located on street', 'characters', 'writing language', 'educated at', 'animator', 'narrative motif', 'enemy of', 'located on terrain feature', 'participant', 'voice actor', 'given name', 'influenced by', 'location of formation', 'has edition or translation', 'parent organization', 'distributed by', 'contributor to the creative work or subject', 'significant event', 'business model', 'form of creative work', 'historic county', 'notable work', 'student', 'dedicated to', 'has cause', 'public holiday', 'media franchise', 'references work, tradition or theory', 'narrative location', 'set in environment', 'character designer', 'Hong Kong film rating', 'MTRCB rating', 'BAMID film rating', 'sexual orientation', 'filming location', 'main subject', 'set during recurring event', 'place served by transport hub', 'work location', 'inspired by', 'allegiance', 'noble title']\n"
     ]
    }
   ],
   "source": [
    "label_list = list(data_repository.get_rel2lbl().values())\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embeddings model over this list such that we can ientify user intent based on the embeddings of a given sentence.\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nltk to remove stop words and lemmatize the labels\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/omkaringale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/omkaringale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/omkaringale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/omkaringale/nltk_data'\n    - '/opt/miniconda3/envs/atai/nltk_data'\n    - '/opt/miniconda3/envs/atai/share/nltk_data'\n    - '/opt/miniconda3/envs/atai/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho is the director of the movie?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m query_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m query_tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m query_tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     10\u001b[0m query_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m query_tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39misalnum()]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/atai/lib/python3.10/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/atai/lib/python3.10/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/atai/lib/python3.10/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/atai/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/atai/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m/opt/miniconda3/envs/atai/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/omkaringale/nltk_data'\n    - '/opt/miniconda3/envs/atai/nltk_data'\n    - '/opt/miniconda3/envs/atai/share/nltk_data'\n    - '/opt/miniconda3/envs/atai/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "query=\"Who is the director of the movie?\"\n",
    "query_tokens = word_tokenize(query)\n",
    "query_tokens = [lemmatizer.lemmatize(token) for token in query_tokens if token not in stop_words]\n",
    "query_tokens = [token for token in query_tokens if token.isalnum()]\n",
    "query = ' '.join(query_tokens)\n",
    "print(query)\n",
    "query_embedding = model.encode([query])\n",
    "print(query_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: who is the director of\n",
      "Category: director, Score: 0.7266\n",
      "Category: art director, Score: 0.6899\n",
      "\n",
      "Query: when was it released\n",
      "Category: publication date, Score: 0.3693\n",
      "Category: original film format, Score: 0.3171\n",
      "\n",
      "Query: how much did it make\n",
      "Category: production company, Score: 0.3031\n",
      "Category: production designer, Score: 0.2906\n",
      "\n",
      "Query: who stars in\n",
      "Category: cast member, Score: 0.5475\n",
      "Category: IMDb ID, Score: 0.5046\n",
      "\n",
      "Query: what type of movie is\n",
      "Category: original film format, Score: 0.5142\n",
      "Category: film editor, Score: 0.4918\n",
      "\n",
      "Query: who is the author of this book\n",
      "Category: author, Score: 0.6468\n",
      "Category: notable work, Score: 0.4575\n",
      "\n",
      "Query: did this movie win any nominations\n",
      "Category: nominated for, Score: 0.6339\n",
      "Category: EIRIN film rating, Score: 0.4111\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "class TextClassifier:\n",
    "    def __init__(self, categories: List[str], model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the classifier with categories and load the embedding model.\n",
    "        \n",
    "        Args:\n",
    "            categories: List of category labels\n",
    "            model_name: Name of the sentence-transformers model to use\n",
    "        \"\"\"\n",
    "        self.categories = categories\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        # Pre-compute embeddings for all categories\n",
    "        self.category_embeddings = self.model.encode(categories)\n",
    "        \n",
    "    def classify(self, query: str, top_k: int = 1) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Classify the input query and return top_k most similar categories with scores.\n",
    "        \n",
    "        Args:\n",
    "            query: Input text to classify\n",
    "            top_k: Number of top matches to return\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples containing (category, similarity_score)\n",
    "        \"\"\"\n",
    "        # Get embedding for the query\n",
    "        query_embedding = self.model.encode([query])[0]\n",
    "        \n",
    "        # Calculate cosine similarity with all category embeddings\n",
    "        similarities = np.dot(self.category_embeddings, query_embedding) / (\n",
    "            np.linalg.norm(self.category_embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
    "        )\n",
    "        \n",
    "        # Get top_k matches\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        return [(self.categories[idx], similarities[idx]) for idx in top_indices]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define categories\n",
    "    categories = label_list\n",
    "    \n",
    "    # Initialize classifier\n",
    "    classifier = TextClassifier(categories)\n",
    "    \n",
    "    # Example queries\n",
    "    test_queries = [\n",
    "        \"who is the director of\",\n",
    "        \"when was it released\",\n",
    "        \"how much did it make\",\n",
    "        \"who stars in\",\n",
    "        \"what type of movie is\",\n",
    "        \"who is the author of this book\",\n",
    "        \"did this movie win any nominations\"\n",
    "    ]\n",
    "    \n",
    "    # Test classification\n",
    "    for query in test_queries:\n",
    "        results = classifier.classify(query, top_k=2)\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        for category, score in results:\n",
    "            print(f\"Category: {category}, Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
